{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb84f88e",
   "metadata": {},
   "source": [
    "### In this notebook, we will demonstrate how we process and label the data and building models to classify buildings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75592a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import gdal\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point, Polygon, LineString\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import sklearn \n",
    "import imblearn\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, auc\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from statistics import mean\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "\n",
    "import rasterio\n",
    "from rasterio import plot as rasterplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fab31f",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d32522",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f05729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagery1 = gdal.Open('Imagery/ImageryAOI1.tif')\n",
    "imagery2 = gdal.Open('Imagery/ImageryAOI2.tif')\n",
    "imagery3 = gdal.Open('Imagery/ImageryAOI3.tif')\n",
    "imagery4 = gdal.Open('Imagery/ImageryAOI4.tif')\n",
    "imagery5 = gdal.Open('Imagery/ImageryAOI5.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5225c4ff",
   "metadata": {},
   "source": [
    "### Convert Imagery to Grid\n",
    "In this step, we use GDAL to split raster data into equal patches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56d86d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point, Polygon, LineString\n",
    "# We examine the dimensions of each imagery.\n",
    "I1 = imagery1.GetGeoTransform()\n",
    "I2 = imagery2.GetGeoTransform()\n",
    "I3 = imagery3.GetGeoTransform()\n",
    "I4 = imagery4.GetGeoTransform()\n",
    "I5 = imagery5.GetGeoTransform()\n",
    "I1,I2,I3,I4,I5 # in m\n",
    "\n",
    "size1 = imagery1.RasterXSize,imagery1.RasterYSize\n",
    "size2 = imagery2.RasterXSize,imagery2.RasterYSize\n",
    "size3 = imagery3.RasterXSize,imagery3.RasterYSize\n",
    "size4 = imagery4.RasterXSize,imagery4.RasterYSize\n",
    "size5 = imagery5.RasterXSize,imagery5.RasterYSize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee8376f",
   "metadata": {},
   "source": [
    "#### Determine the patch size. \n",
    "For consistency, we want the same size for all patches in each imagery. However, it is difficult to find a common divider for the dimensions of all 5 imageries. Hence, we select 20 by 20 as this size provides enough information while avoiding taking up too much computing power. In this case, when each imagery is converted into patches, it is cropped slightly on the rightmost side. \n",
    "\n",
    "You can adjust the patch below for new sets of imageries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b816a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "xsize = 20\n",
    "ysize = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfd24af",
   "metadata": {},
   "source": [
    "#### Slicing and information extraction\n",
    "Below is a demonstration of how we slice the imagery of AOI1 into patches. The process is the same for other imageries except that you need to change approariate variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e167679a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coordinates of imagery 1\n",
    "xmin_1 = I1[0] # top left corner\n",
    "ymax_1 = I1[3] # top left corner\n",
    "res_1 = I1[1] # resolution\n",
    "\n",
    "# Length of imagery 1\n",
    "xlen_1 = res_1 * imagery1.RasterXSize # RasterXSize: how many pixels in the x direction\n",
    "ylen_1 = res_1 * imagery1.RasterYSize\n",
    "\n",
    "# How many patches in each direction\n",
    "import math\n",
    "div_x_1 = math.floor(xlen_1/xsize)\n",
    "div_y_1 = math.floor(ylen_1/ysize)\n",
    "\n",
    "# Loop to find the coordinates of each patch\n",
    "xsteps_1 = [xmin_1 + xsize * i for i in range(div_x_1+1)]\n",
    "ysteps_1 = [ymax_1 - ysize * i for i in range(div_y_1+1)]\n",
    "\n",
    "# Change the working directory to where you want to store the sliced patches\n",
    "%cd Patch_Imagery1\n",
    "\n",
    "# Prepare an output dataframe for storing coordinate and band information of each patch\n",
    "# Columns should include: which coordinates(4), band info mean (8), band info sd (8), AOI(1), Code(1)\n",
    "zeros = np.zeros((div_x_1*div_y_1, 22)) \n",
    "imagery1_df = pd.DataFrame(zeros, columns=['xmin','ymax','xmax','ymin','B1_m','B2_m','B3_m','B4_m','B5_m','B6_m','B7_m','B8_m','B1_std','B2_std','B3_std','B4_std','B5_std','B6_std','B7_std','B8_std', 'AOI','Code'])\n",
    "\n",
    "# In this step, we loop through each patch and perform the following actions:\n",
    "for i in range(div_x_1):\n",
    "    for j in range(div_y_1):\n",
    "        \n",
    "        # 1. Slice patches into tif files and store them in the work directory defined above\n",
    "        xmin = xsteps_1[i] \n",
    "        xmax = xsteps_1[i+1] \n",
    "        ymax = ysteps_1[j]  \n",
    "        ymin = ysteps_1[j+1]\n",
    "        \n",
    "        gdal.Translate('imagery1'+'_'+str(i)+'_'+str(j)+'.tif', imagery1,\n",
    "                      projWin=(xmin,ymax,xmax,ymin))\n",
    "        \n",
    "        # 2. Re-read each patch into python\n",
    "        imagery = gdal.Open('/Users/shurui/Desktop/ALab/Patch_Imagery1/imagery1'+'_'+str(i)+'_'+str(j)+'.tif')\n",
    "\n",
    "        # 3. Extract band information of each patch\n",
    "        B1 = pd.DataFrame(imagery.GetRasterBand(1).ReadAsArray().astype(np.float64))\n",
    "        B2 = pd.DataFrame(imagery.GetRasterBand(2).ReadAsArray().astype(np.float64))\n",
    "        B3 = pd.DataFrame(imagery.GetRasterBand(3).ReadAsArray().astype(np.float64))\n",
    "        B4 = pd.DataFrame(imagery.GetRasterBand(4).ReadAsArray().astype(np.float64))\n",
    "        B5 = pd.DataFrame(imagery.GetRasterBand(5).ReadAsArray().astype(np.float64))\n",
    "        B6 = pd.DataFrame(imagery.GetRasterBand(6).ReadAsArray().astype(np.float64))\n",
    "        B7 = pd.DataFrame(imagery.GetRasterBand(7).ReadAsArray().astype(np.float64))\n",
    "        B8 = pd.DataFrame(imagery.GetRasterBand(8).ReadAsArray().astype(np.float64))\n",
    "\n",
    "        # 4. Put coordinate information into the dataframe\n",
    "        imagery1_df.iloc[i*div_y_1+j,0]=xmin\n",
    "        imagery1_df.iloc[i*div_y_1+j,1]=ymax\n",
    "        imagery1_df.iloc[i*div_y_1+j,2]=xmax\n",
    "        imagery1_df.iloc[i*div_y_1+j,3]=ymin\n",
    "        \n",
    "        # 5. Take average of band infomation of pixels in each patch and put them into the dataframe\n",
    "        imagery1_df.iloc[i*div_y_1+j,4] = np.mean(B1).mean()\n",
    "        imagery1_df.iloc[i*div_y_1+j,5] = np.mean(B2).mean()\n",
    "        imagery1_df.iloc[i*div_y_1+j,6] = np.mean(B3).mean()\n",
    "        imagery1_df.iloc[i*div_y_1+j,7] = np.mean(B4).mean()\n",
    "        imagery1_df.iloc[i*div_y_1+j,8] = np.mean(B5).mean()\n",
    "        imagery1_df.iloc[i*div_y_1+j,9] = np.mean(B6).mean()\n",
    "        imagery1_df.iloc[i*div_y_1+j,10] = np.mean(B7).mean()\n",
    "        imagery1_df.iloc[i*div_y_1+j,11] = np.mean(B8).mean()\n",
    "\n",
    "        # 6. Take standard deviation of band infomation of pixels in each patch and put them into the dataframe\n",
    "        imagery1_df.iloc[i*div_y_1+j,12] = B1.values.flatten().std()\n",
    "        imagery1_df.iloc[i*div_y_1+j,13] = B2.values.flatten().std()\n",
    "        imagery1_df.iloc[i*div_y_1+j,14] = B3.values.flatten().std()\n",
    "        imagery1_df.iloc[i*div_y_1+j,15] = B4.values.flatten().std()\n",
    "        imagery1_df.iloc[i*div_y_1+j,16] = B5.values.flatten().std()\n",
    "        imagery1_df.iloc[i*div_y_1+j,17] = B6.values.flatten().std()\n",
    "        imagery1_df.iloc[i*div_y_1+j,18] = B7.values.flatten().std()\n",
    "        imagery1_df.iloc[i*div_y_1+j,19] = B8.values.flatten().std()\n",
    "\n",
    "        # 7. Put the associated AOI number into the Dataframe\n",
    "        imagery1_df.iloc[i*div_y_1+j,20] = 1\n",
    "        \n",
    "        # 8. Give each patch a code which might be used later\n",
    "        imagery1_df.iloc[i*div_y_1+j,21] = '1_'+str(i)+'_'+str(j)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c72c156",
   "metadata": {},
   "source": [
    "## Labeling\n",
    "In this step, we will label each patch with either 1 or 0. Below is a demonstration of how we label patches in AOI1. The process is the same for other AOIs except that you need to change approariate variable names.\n",
    "\n",
    "If the intersection of building and the patch exceeds a threshold, we will label this patch as 1, indicating that there is a building in this patch. \n",
    "\n",
    "If the intersection of building and the patch is below a threshold, we will label this patch as 0, indicating that there is no building in this patch. \n",
    "\n",
    "The value of the threshold can be defined according to experience or the data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3f1d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframe for this AOI and the building shapefiles\n",
    "AOI1 = imagery1_df.copy()\n",
    "buildings12 = gpd.read_file('AOI 1-2/Buildings/Buildings.shp')\n",
    "\n",
    "# Create a list of polygons for patches in this AOI\n",
    "\n",
    "# The first polygon of AOI pieces\n",
    "tl1 = [AOI1.iloc[0,1],AOI1.iloc[0,2]]\n",
    "tr1 = [AOI1.iloc[0,3],AOI1.iloc[0,2]]\n",
    "bl1 = [AOI1.iloc[0,1],AOI1.iloc[0,4]]\n",
    "br1 = [AOI1.iloc[0,3],AOI1.iloc[0,4]]\n",
    "coords1 = [bl1,br1,tr1,tl1]\n",
    "polygon1 = Polygon(coords1)\n",
    "# Define a geoseries to collect polygons (use the first polygon to initiate the geoseries)\n",
    "polysAOI1 = gpd.GeoSeries([polygon1])\n",
    "\n",
    "# Loop through the remaining patches to accumulate a geoseries\n",
    "for j in range(1,len(AOI1)):\n",
    "    #piece\n",
    "    tl = [AOI1.iloc[j,1],AOI1.iloc[j,2]]\n",
    "    tr = [AOI1.iloc[j,3],AOI1.iloc[j,2]]\n",
    "    bl = [AOI1.iloc[j,1],AOI1.iloc[j,4]]\n",
    "    br = [AOI1.iloc[j,3],AOI1.iloc[j,4]]\n",
    "    coords = [bl,br,tr,tl]\n",
    "    polygon_geom = Polygon(coords)\n",
    "    polyseries = gpd.GeoSeries([polygon_geom])\n",
    "    polysAOI1 = polysAOI1.append(polyseries)\n",
    "    \n",
    "# Similarly, create a list for building polygons\n",
    "polysbuilding12 = gpd.GeoSeries([buildings12.iloc[0,2]]) #(use the first polygon to initiate the geoseries)\n",
    "for i in range(1,len(buildings12)):\n",
    "    polyb = gpd.GeoSeries([buildings12.iloc[i,2]])\n",
    "    polysbuilding12 = polysbuilding12.append(polyb)\n",
    "\n",
    "# Intersection of patch and building\n",
    "dfAOI1 = gpd.GeoDataFrame({'geometry': polysAOI1, '# of patch':list(range(0,len(AOI1)))})\n",
    "dfbuildings12 = gpd.GeoDataFrame({'geometry': polysbuilding12, '# of building':list(range(0,len(buildings12)))})\n",
    "output1 = gpd.overlay(dfAOI1, dfbuildings12, how='intersection')\n",
    "\n",
    "# Compute the area of intersection\n",
    "arealist1 = []\n",
    "for i in range(0,len(output1)):\n",
    "    area = output1.iloc[i,2].area\n",
    "    arealist1.append(area)\n",
    "output1['area'] = arealist1\n",
    "\n",
    "# If we want to determine the label based on a threshold later, we can output the file here\n",
    "#output.to_csv('/Users/shurui/Desktop/ALab/output_intersection.csv')\n",
    "\n",
    "# If we want to determine the label now\n",
    "\n",
    "# Label determination\n",
    "label_list = list()\n",
    "# Define the threshold\n",
    "threshold = 0.0005\n",
    "# Loop through each patch to compare the area of intersection with the threshold\n",
    "for j in range(0,len(AOI1)):\n",
    "    thispatch = output1.loc[output1['# of patch'] == j]\n",
    "    # If this patch does not have any intersection, that is, there is no building, the label is 0\n",
    "    if thispatch.empty == True:\n",
    "        x = 0\n",
    "        label_list.append(x)  \n",
    "    # Sum up the area of intersection of this patch\n",
    "    totalbuildingarea = thispatch.iloc[:,3].sum() \n",
    "    areaofpatch = dfAOI1.iloc[j,0].area\n",
    "    # Compute the fraction of intersection out of total area of this patch\n",
    "    ratio = totalbuildingarea/areaofpatch\n",
    "    # If the fraction exceeds the threshold, label = 1\n",
    "    if ratio >= threshold:\n",
    "        x = 1\n",
    "        label_list.append(x)\n",
    "     # If the fraction does not exceed the threshold, label = 0\n",
    "    else:\n",
    "        x = 0\n",
    "        label_list.append(x)\n",
    "\n",
    "# Combine the label information with coordinate and band information\n",
    "AOI1full = AOI1.copy()\n",
    "AOI1full['fraction']=label_list1\n",
    "\n",
    "# Save this output\n",
    "AOI1full.to_csv('/Users/shurui/Desktop/ALab/AOI1full.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e27037",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "In this step, we will perform 4 models on our labelled data. After examination, we found that there is no building on AOI2. Hence, for class balance, we choose not to use it in our modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23a592b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages and datasets (we've exported datasets after preprocessing)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, auc\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from statistics import mean\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load data\n",
    "df1 = pd.read_csv(\"/content/drive/MyDrive/ALab_team1/Processed Datasets/AOI1full.csv\")\n",
    "df3 = pd.read_csv(\"/content/drive/MyDrive/ALab_team1/Processed Datasets/AOI3full.csv\")\n",
    "df4 = pd.read_csv(\"/content/drive/MyDrive/ALab_team1/Processed Datasets/AOI4full.csv\")\n",
    "df5 = pd.read_csv(\"/content/drive/MyDrive/ALab_team1/Processed Datasets/AOI5full.csv\")\n",
    "train = pd.concat([df1,df3,df5])\n",
    "test = df4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54d1a9a",
   "metadata": {},
   "source": [
    "#### Determine labels\n",
    "We choose to determine labels during modeling, but you can determine them in preprocessing as specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ba8b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Label'] = np.where(train['fraction'] >= 0.02, 1, 0)\n",
    "test['Label'] = np.where(test['fraction'] >= 0.02, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b33a4f",
   "metadata": {},
   "source": [
    "### Data Rebalance\n",
    "Because buildings are very concentrated, there is a class imbalance in our dataset: we have too few positive labels and too many negative labels. Hence we will try 2 rebalancing methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828dc835",
   "metadata": {},
   "source": [
    "#### Rebalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150bfc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler \n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_train_balanced, y_train_balanced = ros.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a57e5c",
   "metadata": {},
   "source": [
    "#### Weighted Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479285b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE \n",
    "from collections import Counter\n",
    "\n",
    "print('Original dataset shape %s' % Counter(y_train))\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_sm, y_train_sm = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18722268",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ca714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    print('Test set accuracy_score', accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "    # confusion matrix\n",
    "    # cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "    # tn, fp, fn, tp = cm.ravel()\n",
    "    # print(\"tn, fp, fn, tp is\", tn, fp, fn, tp)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test.tolist(), y_test_pred.tolist())\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    print(\"AUC\", auc_score)\n",
    "\n",
    "    print(\"Test Confusion Matrix:\")\n",
    "    fig = plot_confusion_matrix(model, X_test, y_test, display_labels=['No Buildings', 'Buildings'], cmap='Greens')\n",
    "    print(fig)\n",
    "\n",
    "    print(classification_report(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b40d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(max_iter=2000)\n",
    "logreg.fit(X_train_balanced, y_train_balanced)\n",
    "y_test_pred = logreg.predict(X_test)\n",
    "evaluate_model(logreg)\n",
    "print(classification_report(y_test, logreg.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b6855b",
   "metadata": {},
   "source": [
    "### CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf1aab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## using Over Sampler\n",
    "model = DecisionTreeClassifier(max_depth=6)\n",
    "model.fit(X_train_sm,y_train_sm)\n",
    "evaluate_model(model)\n",
    "\n",
    "# Decision Tree with cv, sm balanced data\n",
    "model = DecisionTreeClassifier()\n",
    "params = {\n",
    "    'max_depth': [2, 3, 5, 6, 10, 15, 20],\n",
    "    'min_samples_leaf': [5, 10, 20, 50, 100],\n",
    "    'criterion': [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "# cv = GroupShuffleSplit().split(X_train_sm, y_train_sm, 5)\n",
    "cv = TimeSeriesSplit(n_splits=4, test_size=1, gap=2)\n",
    "grid_search = GridSearchCV(estimator=model, \n",
    "                           param_grid=params, \n",
    "                           cv=cv, n_jobs=-1, verbose=1, scoring = \"roc_auc\")\n",
    "\n",
    "grid_search.fit(X_train_balanced, y_train_balanced)\n",
    "model_best = grid_search.best_estimator_\n",
    "evaluate_model(model_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7af1f79",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "There are a number of papers that discuss this issue, but I really suggest reading Using Random Forest to learn Imbalanced Data, that proposes the use of Weighted Gini (or Entropy) to take into account the class distribution, or using a mixture of Under and Over sampling of the classes when bagging decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7c275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMOTE_SRF = RandomForestClassifier(n_estimators=150, random_state=0)\n",
    "#Create Stratified K-fold cross validation\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
    "scoring = ('f1', 'recall', 'precision')\n",
    "\n",
    "#Evaluate SRF model\n",
    "scores = cross_validate(SMOTE_SRF, X_train_sm, y_train_sm, scoring=scoring, cv=cv)\n",
    "#Get average evaluation metrics\n",
    "\n",
    "print('Mean f1: %.3f' % mean(scores['test_f1']))\n",
    "print('Mean recall: %.3f' % mean(scores['test_recall']))\n",
    "print('Mean precision: %.3f' % mean(scores['test_precision']))\n",
    "\n",
    "SMOTE_SRF.fit(X_train, y_train)\n",
    "\n",
    "y_pred = SMOTE_SRF.predict(X_test)\n",
    "\n",
    "fig = plot_confusion_matrix(SMOTE_SRF, X_test, y_test, display_labels=['With Buildings', 'No Buildings'], cmap='Greens')\n",
    "plt.title('Standard Random Forest Confusion Matrix')\n",
    "plt.show()\n",
    "evaluate_model(SMOTE_SRF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfec210",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20d368c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "modelxgb = xgb.XGBClassifier(learning_rate=0.01, max_depth=2)\n",
    "modelxgb.fit(X_train_sm, y_train_sm)\n",
    "y_pred_xgb = modelxgb.predict(X_test)\n",
    "confusion_matrix(y_test, modelxgb.predict(X_test))\n",
    "print(classification_report(y_test, modelxgb.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5571242c",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4749e02",
   "metadata": {},
   "source": [
    "#### Comparison of AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65e46f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1 = model.predict_proba(X_test)[:, 1]\n",
    "fpr_1, tpr_1, _ = metrics.roc_curve(y_test, y_pred_1)\n",
    "auc_1 = round(metrics.roc_auc_score(y_test, y_pred_1), 4)\n",
    "\n",
    "y_pred_2 = logreg.predict_proba(X_test)[:, 1]\n",
    "fpr_2, tpr_2, _ = metrics.roc_curve(y_test, y_pred_2)\n",
    "auc_2 = round(metrics.roc_auc_score(y_test, y_pred_2), 4)\n",
    "\n",
    "y_pred_3 = SMOTE_SRF.predict_proba(X_test)[:, 1]\n",
    "fpr_3, tpr_3, _ = metrics.roc_curve(y_test, y_pred_3)\n",
    "auc_3 = round(metrics.roc_auc_score(y_test, y_pred_3), 4)\n",
    "\n",
    "y_pred_xgb = modelxgb.predict_proba(X_test)[:, 1]\n",
    "fpr_xgb, tpr_xgb, _ = metrics.roc_curve(y_test, y_pred_xgb)\n",
    "auc_xgb = round(metrics.roc_auc_score(y_test, y_pred_xgb), 4)\n",
    "\n",
    "plt.plot(fpr_1, tpr_1, label=\"Decision Tree, AUC=\"+str(auc_1))\n",
    "plt.plot(fpr_2, tpr_2, label=\"Logistic regression, AUC=\"+str(auc_2))\n",
    "plt.plot(fpr_3, tpr_3, label=\"Random Forest, AUC=\"+str(auc_3))\n",
    "plt.plot(fpr_xgb, tpr_xgb, label=\"XGBoost, AUC=\"+str(auc_xgb))\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b407015",
   "metadata": {},
   "source": [
    "#### Building Labels Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472eebc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly, convert all patches with postive labels into shapefiles\n",
    "im = gdal.Open('Patch_Imagery4/imagery4_0_0.tif')\n",
    "X = pd.read_csv(\"imagery4.csv\")\n",
    "y = pd.read_csv('y_pred_log.csv')\n",
    "X['Label'] = y\n",
    "data = X[X['Label']==1]\n",
    "\n",
    "# the first polygon of AOI pieces\n",
    "tl1 = [data.iloc[0,1],data.iloc[0,2]]\n",
    "tr1 = [data.iloc[0,3],data.iloc[0,2]]\n",
    "bl1 = [data.iloc[0,1],data.iloc[0,4]]\n",
    "br1 = [data.iloc[0,3],data.iloc[0,4]]\n",
    "coords1 = [bl1,br1,tr1,tl1]\n",
    "polygon1 = Polygon(coords1)\n",
    "# Define a geoseries to collect polygons (use the first polygon to initiate the geoseries)\n",
    "polys = gpd.GeoSeries([polygon1])\n",
    "\n",
    "for i in range(1,len(data)):\n",
    "    \n",
    "    tl = [data.iloc[i,1],data.iloc[i,2]]\n",
    "    tr = [data.iloc[i,3],data.iloc[i,2]]\n",
    "    bl = [data.iloc[i,1],data.iloc[i,4]]\n",
    "    br = [data.iloc[i,3],data.iloc[i,4]]\n",
    "    coords = [bl,br,tr,tl]\n",
    "    polygon_geom = Polygon(coords)\n",
    "    polyseries = gpd.GeoSeries([polygon_geom])\n",
    "    polys = polys.append(polyseries)\n",
    "    \n",
    "pred = gpd.GeoDataFrame(geometry=polys,crs='epsg:4326')\n",
    "pred.to_file(filename='pred.shp', driver='ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af11c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secondly, visualize\n",
    "tiff = rasterio.open('Imagery/ImageryAOI4.tif')\n",
    "tiff_extent = [tiff.bounds[0], tiff.bounds[2], tiff.bounds[1], tiff.bounds[3]]\n",
    "\n",
    "shapefile = gpd.read_file('pred.shp')\n",
    "#shapefile = shapefile.to_crs('epsg:4326')\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "f.set_size_inches(18.5, 10.5)\n",
    "\n",
    "\n",
    "# plot DEM\n",
    "rasterplot.show(\n",
    "    tiff.read(1),  # use tiff.read(1) with your data\n",
    "    extent=tiff_extent,\n",
    "    ax=ax,\n",
    ")\n",
    "# plot shapefiles\n",
    "shapefile.plot(ax=ax, facecolor='white', edgecolor='white',aspect=1)\n",
    "plt.show()\n",
    "f.savefig('pred.png', dpi=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
